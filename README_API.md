# Frame Glasses AI API

A FastAPI server that provides AI-powered vision analysis for Frame glasses using Moondream AI. This API enables real-time food analysis, restaurant discovery, and maps integration for a complete food-to-restaurant experience.

## ğŸš€ Features

- **Food Analysis**: Analyze food items, estimate calories, and identify nutrients
- **Restaurant Discovery**: Find restaurants in NYC based on food items
- **Maps Integration**: Get Google Maps links for restaurant locations
- **Real-time Capture**: Capture images directly from Frame glasses
- **AI Integration**: Powered by Moondream AI for advanced vision processing
- **Device Management**: Connect, disconnect, and monitor Frame glasses status
- **RESTful API**: Clean, documented API endpoints with automatic OpenAPI docs

## ğŸ½ï¸ Complete Workflow

1. **User takes photo** of food with Frame glasses
2. **AI analyzes** the food item using Moondream
3. **System finds** a restaurant in NYC serving that food
4. **Maps link** is generated for easy navigation
5. **Results displayed** on glasses and returned via API

## ğŸ“‹ Prerequisites

- Python 3.12+
- Frame glasses with Bluetooth connectivity
- Moondream AI API key (get from [Moondream Playground](https://moondream.ai/c/playground))
- Google API key (optional, for enhanced restaurant search)

## ğŸ› ï¸ Installation

1. **Clone and setup the project**:
   ```bash
   git clone <your-repo>
   cd frame-ble-connect
   git checkout feature/init
   ```

2. **Install dependencies**:
   ```bash
   # Using uv (recommended)
   uv sync
   
   # Or using pip
   pip install -r requirements.txt
   ```

3. **Configure environment**:
   ```bash
   cp env.example .env
   # Edit .env and add your API keys
   ```

## ğŸ”§ Configuration

Create a `.env` file with the following variables:

```env
# Server Configuration
HOST=0.0.0.0
PORT=8000
RELOAD=true

# Moondream AI API Key
MOONDREAM_API_KEY=your_actual_api_key_here

# Google API Key (optional - for restaurant search)
GOOGLE_API_KEY=your_google_api_key_here

# Optional: Logging
LOG_LEVEL=info
```

## ğŸš€ Running the API

### Option 1: Using the startup script
```bash
python start_api.py
```

### Option 2: Using uvicorn directly
```bash
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
```

### Option 3: Using uv
```bash
uv run python start_api.py
```

The API will be available at:
- **API**: http://localhost:8000
- **Documentation**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“š API Endpoints

### Device Management

#### `GET /api/v1/device/status`
Get current Frame glasses device status.

#### `POST /api/v1/device/connect`
Connect to Frame glasses.

#### `POST /api/v1/device/disconnect`
Disconnect from Frame glasses.

#### `POST /api/v1/device/capture`
Capture an image from Frame glasses.

#### `POST /api/v1/device/display`
Display text on Frame glasses.

### AI Analysis

#### `POST /api/v1/analysis/food`
Analyze food items in an uploaded image.

#### `POST /api/v1/analysis/general`
Perform general image analysis with custom prompt.

#### `POST /api/v1/analysis/food-to-restaurant` â­ **NEW**
Complete workflow: Analyze food â†’ Find restaurant â†’ Return maps link.

**Request**: Multipart form with image file.

**Response**:
```json
{
  "success": true,
  "food_analysis": {
    "primary_food_item": "pizza",
    "structured_data": { ... }
  },
  "restaurant_found": true,
  "restaurant": {
    "name": "Joe's Pizza",
    "address": "123 Bleecker St, New York, NY 10012",
    "rating": 4.5,
    "price_level": "$$",
    "maps_link": "https://maps.google.com/?q=Joe's+Pizza+123+Bleecker+St+New+York+NY"
  },
  "maps_link": "https://maps.google.com/?q=Joe's+Pizza+123+Bleecker+St+New+York+NY",
  "primary_food_item": "pizza",
  "processing_time_ms": 1250
}
```

#### `POST /api/v1/analysis/capture-and-analyze`
Capture image from glasses and analyze for food items.

#### `POST /api/v1/analysis/capture-and-find-restaurant` â­ **NEW**
Complete workflow: Capture from glasses â†’ Analyze food â†’ Find restaurant â†’ Display on glasses.

## ğŸ½ï¸ Food-to-Restaurant Workflow

### Option 1: Upload Image
1. **Upload food image**: `POST /api/v1/analysis/food-to-restaurant`
2. **Get restaurant info**: Returns restaurant details + maps link
3. **Navigate**: Use the maps link on your phone

### Option 2: Capture from Glasses
1. **Capture and analyze**: `POST /api/v1/analysis/capture-and-find-restaurant`
2. **Results displayed**: Restaurant name appears on glasses
3. **Get full results**: API returns complete data including maps link

## ğŸ” Example Usage

### Complete food-to-restaurant workflow:

```bash
# 1. Connect to glasses
curl -X POST http://localhost:8000/api/v1/device/connect

# 2. Capture, analyze, and find restaurant
curl -X POST http://localhost:8000/api/v1/analysis/capture-and-find-restaurant \
  -F "resolution=720"

# 3. Check the response for restaurant info and maps link
```

### Using Python requests:

```python
import requests

base_url = "http://localhost:8000/api/v1"

# Complete workflow
response = requests.post(
    f"{base_url}/analysis/capture-and-find-restaurant",
    data={"resolution": 720}
)

result = response.json()
print(f"Found: {result['restaurant']['name']}")
print(f"Maps: {result['maps_link']}")
```

## ğŸ—ï¸ Project Structure

```
frame-ble-connect/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI server
â”‚   â”‚   â”œâ”€â”€ main.py            # Main FastAPI app
â”‚   â”‚   â”œâ”€â”€ routes/            # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py    # AI analysis & restaurant routes
â”‚   â”‚   â”‚   â””â”€â”€ device.py      # Device management routes
â”‚   â”‚   â”œâ”€â”€ models/            # Pydantic models
â”‚   â”‚   â”‚   â”œâ”€â”€ requests.py    # Request models
â”‚   â”‚   â”‚   â””â”€â”€ responses.py   # Response models
â”‚   â”‚   â””â”€â”€ services/          # Business logic
â”‚   â”‚       â”œâ”€â”€ frame_service.py # Frame glasses service
â”‚   â”‚       â””â”€â”€ restaurant_service.py # Restaurant search service
â”‚   â”œâ”€â”€ ai/                    # AI processing
â”‚   â”‚   â””â”€â”€ processors/
â”‚   â”‚       â””â”€â”€ moondream_processor.py # Moondream AI integration
â”‚   â”œâ”€â”€ connect/               # Existing Frame connection code
â”‚   â””â”€â”€ utils/                 # Shared utilities
â”œâ”€â”€ start_api.py              # API startup script
â”œâ”€â”€ env.example               # Environment configuration example
â””â”€â”€ README_API.md            # This file
```

## ğŸ• Supported Food Items

The restaurant service includes mock data for common NYC restaurants:

- **Pizza**: Joe's Pizza (Bleecker St)
- **Sushi**: Sushi Nakazawa (Commerce St)
- **Burger**: Shake Shack (Madison Square Park)
- **Pasta**: Carbone (Thompson St)
- **Taco**: Los Tacos No. 1 (43rd St)

For other food items, a generic search is performed.

## ğŸ”§ Development

### Running tests:
```bash
pytest
```

### Code formatting:
```bash
ruff format src/
```

### Linting:
```bash
ruff check src/
```

## ğŸš¨ Troubleshooting

### Common Issues:

1. **Import errors**: Make sure you're running from the project root
2. **Frame connection fails**: Ensure glasses are powered on and Bluetooth is enabled
3. **Moondream API errors**: Verify your API key is correct and has sufficient credits
4. **Restaurant search fails**: Check if Google API key is set (optional for MVP)

### Debug Mode:
```bash
LOG_LEVEL=debug python start_api.py
```

## ğŸ“ TODO

- [ ] Integrate real Google Places API for restaurant search
- [ ] Add WebSocket support for real-time streaming
- [ ] Implement image caching and optimization
- [ ] Add authentication and rate limiting
- [ ] Improve food analysis parsing
- [ ] Add support for multiple AI providers
- [ ] Implement batch processing
- [ ] Add metrics and monitoring
- [ ] Add user location detection
- [ ] Implement restaurant ratings and reviews

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details. 